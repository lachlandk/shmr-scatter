{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from astropy.cosmology import Planck15\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "\n",
    "import illustris_python as il\n",
    "\n",
    "print(np.version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.6774\n",
    "kpc_to_km = 3.086e16\n",
    "\n",
    "# define base path for data\n",
    "path = \"../../sims.TNG/TNG100-1/output/\"\n",
    "\n",
    "lower_limit = 10**11 * h / 1e10\n",
    "upper_limit = 10**12.5 * h / 1e10\n",
    "\n",
    "# num = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.68it/s]\n"
     ]
    }
   ],
   "source": [
    "scale_factor = np.ndarray(100)\n",
    "redshift = np.ndarray(100)\n",
    "\n",
    "for snapshot in tqdm(range(100)):\n",
    "    snap = il.groupcat.loadHeader(path, snapshot)\n",
    "    \n",
    "    scale_factor[snapshot] = snap[\"Time\"]  # a\n",
    "    redshift[snapshot] = snap[\"Redshift\"]  # z\n",
    "    \n",
    "time = np.round(Planck15.age(redshift), decimals=3)\n",
    "\n",
    "with h5py.File(\"cosmology.hdf5\", \"w\") as file:\n",
    "    file.create_dataset(\"a\", data=scale_factor)\n",
    "    file.create_dataset(\"z\", data=redshift)\n",
    "    file.create_dataset(\"t\", data=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary subhalo IDs\n",
    "group_first_sub_ids = il.groupcat.loadHalos(path, 99, fields=[\"GroupFirstSub\"])\n",
    "group_first_sub_ids = group_first_sub_ids[group_first_sub_ids > -1]\n",
    "\n",
    "# select centrals in desired mass range\n",
    "central_halo_mass = il.groupcat.loadSubhalos(path, 99, fields=[\"SubhaloMassType\"])[:,1]\n",
    "group_first_sub_ids = group_first_sub_ids[(central_halo_mass[group_first_sub_ids] > lower_limit) * (central_halo_mass[group_first_sub_ids] < upper_limit)]\n",
    "num = len(group_first_sub_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load merger trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f3938cd37d40a8840a937b793d5f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14023/14023 [00:14<00:00, 962.64it/s] \n"
     ]
    }
   ],
   "source": [
    "def loadTrees(index):\n",
    "    while True:\n",
    "        tree = il.sublink.loadTree(path, 99, group_first_sub_ids[index], fields=[\"SubhaloGrNr\", \"SnapNum\", \"SubhaloMassType\", \"SubhaloBHMdot\"], onlyMPB=True)\n",
    "        if tree is None:\n",
    "            group_first_sub_ids.pop(index)\n",
    "        else:\n",
    "            break\n",
    "    return tree\n",
    "    \n",
    "\n",
    "trees = process_map(loadTrees, range(num), max_workers=100, chunksize=10)\n",
    "with h5py.File(\"trees.hdf5\", \"w\") as file:\n",
    "    for index, tree in enumerate(tqdm(trees)):\n",
    "        group = file.create_group(str(index))\n",
    "        group.create_dataset(\"SubhaloGrNr\", data=tree[\"SubhaloGrNr\"])\n",
    "        group.create_dataset(\"SnapNum\", data=tree[\"SnapNum\"])\n",
    "        group.create_dataset(\"SubhaloMassType\", data=tree[\"SubhaloMassType\"])\n",
    "        group.create_dataset(\"SubhaloBHMdot\", data=tree[\"SubhaloBHMdot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [35:18<00:00, 21.19s/it]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"snapshots.hdf5\", \"w\") as file:\n",
    "    for snapshot in tqdm(range(100)):\n",
    "        subhalos = il.groupcat.loadSubhalos(path, snapshot, fields=[\"SubhaloFlag\", \"SubhaloGrNr\", \"SubhaloPos\", \"SubhaloVel\"])\n",
    "        \n",
    "        true_halos = subhalos[\"SubhaloFlag\"] == True\n",
    "        group = file.create_group(str(snapshot))\n",
    "        group.create_dataset(\"SubhaloGrNr\", data=subhalos[\"SubhaloGrNr\"][true_halos])\n",
    "        group.create_dataset(\"SubhaloPos\", data=subhalos[\"SubhaloPos\"][true_halos])\n",
    "        group.create_dataset(\"SubhaloVel\", data=subhalos[\"SubhaloVel\"][true_halos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find satellites for each central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [39:14<00:00, 23.54s/it]\n"
     ]
    }
   ],
   "source": [
    "def findSatellites(snapshot, index):\n",
    "    with h5py.File(\"trees.hdf5\", \"r\") as trees:\n",
    "        tree = trees[str(index)]\n",
    "        if snapshot in np.array(tree[\"SnapNum\"]):\n",
    "            index_at_snapshot = tree[\"SubhaloGrNr\"][np.array(tree[\"SnapNum\"]) == snapshot][0]\n",
    "            return list((subhalos == index_at_snapshot).nonzero()[0])\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "# find satellites of every central at every snapshot\n",
    "with h5py.File(\"snapshots.hdf5\", \"r\") as snapshots, h5py.File(f\"satellites.hdf5\", \"w\") as satellites:\n",
    "    for snapshot in tqdm(range(100)):\n",
    "            subhalos = np.array(snapshots[str(snapshot)][\"SubhaloGrNr\"])\n",
    "            position = snapshots[str(snapshot)][\"SubhaloPos\"]\n",
    "            velocity = snapshots[str(snapshot)][\"SubhaloVel\"]\n",
    "            \n",
    "            with Pool(processes=100) as pool:\n",
    "                satellite_indices = pool.starmap(findSatellites, zip(repeat(snapshot), range(len(group_first_sub_ids))), chunksize=100)\n",
    "                  \n",
    "            snap = satellites.create_group(str(snapshot))\n",
    "            for index in range(num):\n",
    "                group = snap.create_group(str(index))\n",
    "                if len(satellite_indices[index]) != 0:\n",
    "                    group.create_dataset(\"SubhaloPos\", data=position[satellite_indices[index]])\n",
    "                    group.create_dataset(\"SubhaloVel\", data=velocity[satellite_indices[index]])\n",
    "                else:\n",
    "                    group.create_dataset(\"SubhaloPos\", data=[])\n",
    "                    group.create_dataset(\"SubhaloVel\", data=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be3f97adfbb4a728e56bec6ce37625c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 455.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def loadGroupKinematics(index):\n",
    "    with h5py.File(\"trees.hdf5\", \"r\") as trees:\n",
    "        history = np.full((2, 100, 3), np.nan)\n",
    "        for snapshot in range(100):\n",
    "            if snapshot in np.array(trees[str(index)][\"SnapNum\"]):\n",
    "                group_index_at_snapshot = trees[str(index)][\"SubhaloGrNr\"][np.array(trees[str(index)][\"SnapNum\"]) == snapshot][0]\n",
    "                group = il.groupcat.loadSingle(path, snapshot, haloID=group_index_at_snapshot)\n",
    "                history[0, snapshot] = group[\"GroupPos\"]\n",
    "                history[1, snapshot] = group[\"GroupVel\"]\n",
    "    return history\n",
    "\n",
    "\n",
    "kinematics = np.array(process_map(loadGroupKinematics, range(num), max_workers=100, chunksize=10))\n",
    "with h5py.File(\"groups.hdf5\", \"w\") as file:\n",
    "    for snapshot in tqdm(range(100)):\n",
    "        snap = file.create_group(str(snapshot))\n",
    "        snap.create_dataset(\"GroupPos\", data=kinematics[:,0,snapshot])\n",
    "        snap.create_dataset(\"GroupVel\", data=kinematics[:,1,snapshot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate quantities from cached data\n",
    "\n",
    "### Central masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8dc55bad324b67b8b80b7ebe032606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadMass(index):\n",
    "    with h5py.File(\"trees.hdf5\", \"r\") as trees:\n",
    "        tree = trees[str(index)]\n",
    "        history = np.full((100, 6), np.nan)\n",
    "        for snapshot in range(100):\n",
    "            if snapshot in np.array(tree[\"SnapNum\"]):\n",
    "                history[snapshot] = np.array(tree[\"SubhaloMassType\"])[np.array(tree[\"SnapNum\"]) == snapshot]\n",
    "    return history\n",
    "\n",
    "\n",
    "halo_mass = np.array(process_map(loadMass, range(num), max_workers=100, chunksize=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14023/14023 [00:03<00:00, 4554.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for halo in tqdm(halo_mass):\n",
    "    for snapshot in range(100):\n",
    "        if snapshot > 0 and np.isnan(halo[snapshot, 1]) and not np.isnan(halo[snapshot - 1, 1]):\n",
    "            halo[snapshot, 1] = (halo[snapshot - 1, 1] + halo[snapshot + 1, 1]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black hole accretion rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d59495b8984faeb2864cfa9416eb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadbhmdot(index):\n",
    "    with h5py.File(\"trees.hdf5\", \"r\") as trees:\n",
    "        tree = trees[str(index)] \n",
    "        history = np.full(100, np.nan)\n",
    "        for snapshot in range(100):\n",
    "            if snapshot in np.array(tree[\"SnapNum\"]):\n",
    "                history[snapshot] = tree[\"SubhaloBHMdot\"][np.array(tree[\"SnapNum\"]) == snapshot]\n",
    "    return history\n",
    "\n",
    "\n",
    "bh_mdot = np.array(process_map(loadbhmdot, range(num), max_workers=100, chunksize=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001e85db7ce2464a8ac4ef6586df1fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculateSatellites(index):\n",
    "    with h5py.File(\"trees.hdf5\", \"r\") as trees, h5py.File(f\"satellites.hdf5\", \"r\") as subhalos:\n",
    "        history = np.full(100, np.nan)\n",
    "        for snapshot in range(100):\n",
    "            if snapshot in np.array(trees[str(index)][\"SnapNum\"]):\n",
    "                history[snapshot] = subhalos[str(snapshot)][str(index)][\"SubhaloPos\"].shape[0] - 1\n",
    "    return history\n",
    "\n",
    "\n",
    "satellites = np.array(process_map(calculateSatellites, range(num), max_workers=100, chunksize=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the radial and tangential velocities\n",
    "Suppose we have some velocity vector in cartesian coordinates:\n",
    "$$\\vec{v}=v_x\\hat{x}+v_y\\hat{y}+v_z\\hat{z}.$$\n",
    "We want to convert this to a representation in spherical coordinates, given by\n",
    "$$\\vec{v}=v_r\\hat{r}+v_\\theta\\hat{\\theta}+v_\\phi\\hat{\\phi}.$$\n",
    "\n",
    "Then the radial velocity component is simply $v_r$, and the tangential velocity component is given by\n",
    "$$v_\\text{tan}=\\sqrt{v_\\theta^2+v_\\phi^2}.$$\n",
    "\n",
    "Notice that we can extract the spherical components by projecting onto the spherical unit vectors.\n",
    "For this we need the dot products between cartesian and spherical unit vectors:\n",
    "$$\\hat{x}\\cdot\\hat{r}=\\sin\\theta\\cos\\phi, \\quad \\hat{y}\\cdot\\hat{r}=\\sin\\theta\\sin\\phi, \\quad \\hat{z}\\cdot\\hat{r}=\\cos\\theta,$$\n",
    "$$\\hat{x}\\cdot\\hat{\\theta}=\\cos\\theta\\cos\\phi, \\quad \\hat{y}\\cdot\\hat{\\theta}=\\cos\\theta\\sin\\phi, \\quad \\hat{z}\\cdot\\hat{\\theta}=-\\sin\\theta,$$\n",
    "$$\\hat{x}\\cdot\\hat{\\phi}=-\\sin\\phi, \\quad \\hat{y}\\cdot\\hat{\\phi}=\\cos\\phi, \\quad \\hat{z}\\cdot\\hat{\\phi}=0.$$\n",
    "\n",
    "Hence the spherical components are\n",
    "$$v_r=\\vec{v}\\cdot\\hat{r}=v_x\\sin\\theta\\cos\\phi+v_y\\sin\\theta\\sin\\phi+v_z\\cos\\theta,$$\n",
    "$$v_\\theta=\\vec{v}\\cdot\\hat{\\theta}=v_x\\cos\\theta\\cos\\phi+v_y\\cos\\theta\\sin\\phi-v_z\\sin\\theta,$$\n",
    "$$v_\\phi=\\vec{v}\\cdot\\hat{\\phi}=-v_x\\sin\\phi+v_y\\cos\\phi.$$\n",
    "\n",
    "Now, bearing in mind the ordinary coordinate transformations:\n",
    "$$x=r\\sin\\theta\\cos\\phi, \\quad y=r\\sin\\theta\\sin\\phi, \\quad z=r\\cos\\theta$$\n",
    "$$\\implies r=\\sqrt{x^2+y^2+z^2}, \\quad \\rho=\\sqrt{x^2+y^2}=r\\sin\\theta,$$\n",
    "we get\n",
    "$$v_r=\\vec{v}\\cdot\\hat{r}=v_x\\frac{x}{r}+v_y\\frac{y}{r}+v_z\\frac{z}{r},$$\n",
    "$$v_\\theta=\\vec{v}\\cdot\\hat{\\theta}=v_x\\frac{xz}{\\rho r}+v_y\\frac{yz}{\\rho r}-v_z\\frac{\\rho}{r},$$\n",
    "$$v_\\phi=\\vec{v}\\cdot\\hat{\\phi}=-v_x\\frac{y}{\\rho}+v_y\\frac{x}{\\rho}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b632deac6e840148b7cd545af269b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14023 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loadVelocities(index):\n",
    "    with h5py.File(\"trees.hdf5\", \"r\") as trees, h5py.File(f\"satellites.hdf5\", \"r\") as subhalos, h5py.File(\"groups.hdf5\", \"r\") as groups:\n",
    "        history = np.full(100, np.nan)\n",
    "        for snapshot in range(100):\n",
    "            if snapshot in np.array(trees[str(index)][\"SnapNum\"]) and len(subhalos[str(snapshot)][str(index)][\"SubhaloPos\"]) > 1:\n",
    "                    \n",
    "                x = subhalos[str(snapshot)][str(index)][\"SubhaloPos\"][1:] * scale_factor[snapshot] * kpc_to_km / h  # [km]\n",
    "                v = subhalos[str(snapshot)][str(index)][\"SubhaloVel\"][1:]  # [km/s]\n",
    "                \n",
    "                x_com = groups[str(snapshot)][\"GroupPos\"][index] * scale_factor[snapshot] * kpc_to_km / h  # [km]\n",
    "                v_com = groups[str(snapshot)][\"GroupVel\"][index] / scale_factor[snapshot]  # [km/s]\n",
    "\n",
    "                x -= x_com\n",
    "                v -= v_com\n",
    "                \n",
    "                # at some point between numpy v1.19.5 and v1.24.3 the handling of dtypes has changed so this is now required\n",
    "                x = x.astype(\"float64\")\n",
    "                v = v.astype(\"float64\")\n",
    "                \n",
    "                rho = np.sqrt(x[:,0]**2 + x[:,1]**2)\n",
    "                r = np.sqrt(rho**2 + x[:,2]**2)\n",
    "\n",
    "                v_r = (v[:,0]*x[:,0] + v[:,1]*x[:,1] + v[:,2]*x[:,2]) / r\n",
    "                theta_dot = ((v[:,0]*x[:,0]*x[:,2] + v[:,1]*x[:,1]*x[:,2]) / rho - v[:,2]*rho) / r\n",
    "                phi_dot = (-v[:,0]*x[:,1] + v[:,1]*x[:,0]) / rho\n",
    "                v_tan_2 = theta_dot**2 + phi_dot**2\n",
    "\n",
    "                beta = 1 - v_tan_2 / (2*v_r**2)\n",
    "\n",
    "                history[snapshot] = np.median(beta)\n",
    "    return history\n",
    "\n",
    "\n",
    "beta = np.array(process_map(loadVelocities, range(num), max_workers=100, chunksize=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to correct units and save\n",
    "bh_mdot_scaled = bh_mdot * 1e10 / 0.978  # [M_sol/Gyr]\n",
    "\n",
    "halo_mass[halo_mass == 0] = 1e-15\n",
    "halo_mass_scaled = np.log10(halo_mass * 1e10 / h)  # [log10(M_sol)]\n",
    "\n",
    "with h5py.File(\"halo_properties.hdf5\", \"w\") as file:\n",
    "    file.create_dataset(\"bh_mdot\", data=bh_mdot_scaled)\n",
    "    file.create_dataset(\"satellites\", data=satellites)\n",
    "    file.create_dataset(\"beta\", data=beta)\n",
    "    file.create_dataset(\"Mh\", data=halo_mass_scaled[:,:,1])\n",
    "    file.create_dataset(\"Ms\", data=halo_mass_scaled[:,:,4])\n",
    "    file.create_dataset(\"Mg\", data=halo_mass_scaled[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
